#!/usr/bin/python3

from lxml import html

import argparse
import http.cookiejar
import json
import os
import re
import requests
import string
import time

API_URL = 'https://bcourses.berkeley.edu/api/v1/'

ONE_MB = 2**20
ONE_KB = 2**10

FILE_COUNT = 0
TOTAL_SIZE = 0

def get_valid_filename(s):
    s = str(s).strip().replace(' ', '_')
    return re.sub(r'(?u)[^-\w.]', '', s)

def get_file_size(size):
    """
    Returns the file size in a human readable format. (e.g. 700.79 KB, 19.55 MB)
    """
    if size >= ONE_MB:
        return '{:.2f} MB'.format(size / ONE_MB)
    elif size >= ONE_KB:
        return '{:.2f} KB'.format(size / ONE_KB)
    return '{} {}'.format(size, 'bytes')

def write_file(url, filename, path, size, cookies, depth):
    """
    Writes a file to disk.
    """
    global TOTAL_SIZE
    global FILE_COUNT
    TOTAL_SIZE += size
    FILE_COUNT += 1
    readable_size = ''
    if size != 0:
        readable_size = get_file_size(size)

    print("{0:>9} {1}{2}".format(readable_size, '|-- ' * depth, filename))

    file = requests.get(url, cookies=cookies, stream=True)
    filename = get_valid_filename(filename)
    with open(path + '/' + filename, 'wb') as f:
        for block in file.iter_content(1024):
            f.write(block)

def download_file(file, path, cookies, depth):
    """
    Downloads a file given a canvas file object.
    """
    if not file or not file['url']:
        return 0

    write_file(file['url'], file['display_name'], path, file['size'], cookies, depth)
    return 0

def download_page(html_text, path, name, cookies, depth):
    """
    Scrapes a standard page for all files on it and downloads them.
    """
    files = html_text.find_class('instructure_file_link')
    if files:
        print("{0:9} {1}{2}/".format('', '|-- ' * depth, name))
        try:
            path = path + '/' + name
            os.mkdir(path)
        except:
            print('Failed to create an output directory.')
            return 1

    for f in files:
        file_id = re.search('\/courses\/[0-9]*\/files\/([0-9]*)\/', f.get('href')).group(1)
        file_api = API_URL + 'files/' +  file_id
        file_response = json.loads(requests.get(file_api, cookies=cookies).text[9:])

        download_file(file_response, path, cookies, depth + 1)
    return 0

def download_quiz(quiz, path, cookies, depth):
    if not quiz or 'html_url' not in quiz or not quiz['html_url']:
        return 0

    name = get_valid_filename(quiz['title'])
    request = quiz['html_url']
    response = requests.get(request, cookies=cookies)

    if not response:
        return 0

    download_page(html.document_fromstring(response.content), path, name, cookies, depth)
    return 0

def download_module_body(page, path, cookies, depth):
    """
    Parses the body text of a module and finds files to download.
    """
    if not page or not 'body' in page or not page['body']:
        return 0

    name = get_valid_filename(page['title'])

    download_page(html.document_fromstring(page['body']), path, name, cookies, depth)
    return 0

def download_module(module, path, cookies, depth):
    """
    Downloads all the files in a given canvas module object.
    """
    if not 'items_url' in module or not 'name' in module:
        return 0

    api_request = module['items_url'] + '?per_page=200'
    name = get_valid_filename(module['name'])

    response = json.loads(requests.get(api_request, cookies=cookies).text[9:])
    if not response:
        return 0

    print("{0:9} {1}{2}/".format('', '|-- ' * depth, name))
    try:
        path = path + '/' + name
        os.mkdir(path)
    except:
        print('Failed to create an output directory.')
        return 1

    for r in response:
        if 'url' in r:
            object = json.loads(requests.get(r['url'], cookies=cookies).text[9:])
            if 'body' in object:
                # Object is a page: scrape the page for files
                download_module_body(object, path, cookies, depth + 1)
            elif 'quiz_type' in object:
                # Object is a quiz: scrape the quiz for files
                download_quiz(object, path, cookies, depth + 1)
            else:
                # Object is a file: download the file
                download_file(object, path, cookies, depth + 1)
    return 0

def download_modules(course_id, path, cookies, depth=1):
    """
    Recursively downloads all the modules in a course.
    """
    api_request = API_URL + 'courses/' + course_id + '/modules?per_page=200'
    response = json.loads(requests.get(api_request, cookies=cookies).text[9:])
    if not response:
        return 0

    print("{0:9} {1}Modules/".format('', '|-- ' * depth))
    try:
        os.mkdir(path)
    except:
        print('Failed to create an output directory.')
        return 1

    for r in response:
        download_module(r, path, cookies, depth + 1)
    return 0

def download_assignment(assignment_url, path, cookies, depth):
    """
    Downloads all the files on a given assignment.
    """
    response = requests.get(assignment_url, cookies=cookies)
    if not response:
        return 0

    download_page(html.document_fromstring(response.content), path, name, cookies, depth)
    return 0

def download_assignments(course_id, path, cookies, depth=1):
    """
    Recursively downloads all assignments for a course given a canvas course id.
    """
    api_request = API_URL + 'courses/' + course_id + '/students/submissions?per_page=200'
    response = json.loads(requests.get(api_request, cookies=cookies).text[9:])
    if not response:
        return 0

    print("{0:9} {1}Assignments/".format('', '|-- ' * depth))
    try:
        os.mkdir(path)
    except:
        print('Failed to create an output directory.')
        return 1

    for r in response:
        assignment_url = 'https://bcourses.berkeley.edu/courses/%s/assignments/%s' % (course_id, r['assignment_id'])
        download_assignment(assignment_url, path, cookies, depth+1)
    return 0

def download_files(api_request, path, cookies, depth):
    """
    Downloads all files in a folder given a file list canvas api request.
    """
    if not api_request:
        return 0

    response = json.loads(requests.get(api_request, cookies=cookies).text[9:])

    for r in response:
        download_file(r, path, cookies, depth)
    return 0

def download_folder(api_response, path, cookies, depth=1, root=False):
    """
    Recursively downloads a directory tree given a canvas folder object.
    """
    if not api_response or not 'folders_url' in api_response:
        return 0

    if root:
        print("{0:9} {1}Files/".format('', '|-- ' * depth))
        try:
            os.mkdir(path)
        except:
            print('Failed to create an output directory.')
            return 1
    else:
        print("{0:9} {1}{2}/".format('', '|-- ' * depth, api_response['name']))
        path = path + '/' + get_valid_filename(api_response['name'])
        try:
            os.mkdir(path)
        except:
            print('Failed to create an output directory.')
            return 1

    response = json.loads(requests.get(api_response['folders_url'], cookies=cookies).text[9:])
    download_files(api_response['files_url'], path, cookies, depth + 1)

    for r in response:
        download_folder(r, path, cookies, depth + 1)
    return 0

def download_root(name, asset_string, path, cookies):
    """
    Recursively downloads all content for a course.
    """
    print("{0:9} {1}/".format('', name))

    path = path + '/' + get_valid_filename(name)
    try:
        os.mkdir(path)
    except:
        print('Failed to create an output directory.')
        return 1

    asset_attr = asset_string.split('_')
    api_req = API_URL + asset_attr[0] + 's/' + asset_attr[1] + '/' + 'folders/root?per_page=200'
    response = json.loads(requests.get(api_req, cookies=cookies).text[9:])

    # download_folder(response, path + '/Files', cookies, root=True)

    if asset_attr[0] == 'course':
        # download_assignments(asset_attr[1], path + '/Assignments', cookies)
        download_modules(asset_attr[1], path + '/Modules', cookies)
    return 0

def download(args):
    cookies = http.cookiejar.MozillaCookieJar(args.cookies)
    try:
        cookies.load(ignore_expires=True)
    except:
        print('Failed to load the cookie file.\nMake sure the name of the file was entered correctly and that the first line of the cookie file is \'# HTTP Cookie File\'.')
        return 1

    for c in cookies:
        c.expires = time.time() + (24 * 3600)

    response = requests.get('https://bcourses.berkeley.edu/files', cookies=cookies)
    if response.url != 'https://bcourses.berkeley.edu/files':
        print('Failed to access your bCourses files.')
        return 1

    path = args.output
    try:
        os.mkdir(path)
    except FileExistsError:
        print('The directory %s already exists.' % path)
        return 1
    except:
        print('Failed to create an output directory.')
        return 1

    # Scrapes files page for courses, groups, and user's files.
    # Messier than making API calls for courses and groups but easier.
    folders = json.loads(html.document_fromstring(response.content).get_element_by_id('application').findtext('script').split(';')[1][9:])['FILES_CONTEXTS']
    courses = {}
    for f in folders:
        courses[f['asset_string']] = f['name']

    # Scrape courses page for missing courses since
    # the API doesn't return 'inactive' courses.
    courses_response = requests.get('https://bcourses.berkeley.edu/courses', cookies=cookies)
    courses_response = html.document_fromstring(courses_response.content).find_class('course-list-table-row')
    for c in courses_response:
        a = c.find_class('course-list-course-title-column')[0].find('a')
        if a is not None:
            asset_string = 'course_' + a.get('href').split('/')[2]
            name = a.get('title')
            if asset_string not in courses:
                courses[asset_string] = name

    for asset_string, name in courses.items():
        download_root(name, asset_string, path, cookies)
        print('')

    print('%s files downloaded (%s)' % (FILE_COUNT, get_file_size(TOTAL_SIZE)))
    return 0

def main(argv=None):
    parser = argparse.ArgumentParser(description='Downloads a complete backup of bCourses.')
    parser.usage = """
    You need to supply a cookie.txt from your bCourses session.
    This can be done, for example, by using the cookies.txt extension in Google Chrome.
    You can then grab your cookies.txt file from https://bcourses.berkeley.edu/."""

    parser.add_argument('--cookies', type=str, default='cookies.txt', help='Path to the cookiefile.')
    parser.add_argument('--output', type=str, default='downloads', help='Path to the output directory.')
    args = parser.parse_args(argv)

    return download(args)

if __name__ == '__main__':
    main()
